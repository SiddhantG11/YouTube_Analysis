import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re
import nltk
import ast
import matplotlib.pyplot as plt
pd.set_option('display.max_columns', None)
pd.set_option('max_colwidth', None)


df = pd.read_csv('datasets/analysis.csv')


# no nul' values 
df.isna().sum()


df['reordered_scores'] = df['reordered_scores'].apply((lambda x: ast.literal_eval(x)))


fears = df.sort_values(by='reordered_scores', key=lambda x: x.apply(lambda y: y[0]), ascending=False)
frustrations = df.sort_values(by='reordered_scores', key=lambda x: x.apply(lambda y: y[1]), ascending=False)
aspirations = df.sort_values(by='reordered_scores', key=lambda x: x.apply(lambda y: y[2]), ascending=False)



fears





# Preprocess the text
def preprocess_text(text):
    text = re.sub(r'\W', ' ', str(text))
    text = text.lower()
    text = re.sub(r'\s+[a-zA-Z]\s+', ' ', text)
    text = re.sub(r'\^[a-zA-Z]\s+', ' ', text)
    text = re.sub(r'\s+', ' ', text, flags=re.I)
    text = re.sub(r'^b\s+', '', text)
    return text


df['processed_comments'] = df['split_comments'].apply(preprocess_text)


lemmatizer = WordNetLemmatizer()
def lemmatize_text(text):
    words = nltk.word_tokenize(text)
    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
    return ' '.join(lemmatized_words)


df['processed_comments'] = df['processed_comments'].apply(lemmatize_text)



# Vectorize the text
tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), max_features=1000)
X = tfidf_vectorizer.fit_transform(df['processed_comments'])



# Determine the optimal number of clusters using the elbow method
wcss = []
for i in range(1, 11):  # Try different numbers of clusters
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)



# Plot the elbow curve
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()


optimal_clusters = 4



# Cluster the data with the optimal number of clusters
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
clusters = kmeans.fit_predict(X)


df['cluster'] = clusters


frustrations.head(20)









